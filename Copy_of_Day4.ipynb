{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Day4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuppohekar/Deep-Learning-Projects/blob/main/Copy_of_Day4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs2RDEBtwGCc"
      },
      "source": [
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.layers import Embedding\n",
        "from tensorflow.python.keras.layers import SeparableConv1D\n",
        "from tensorflow.python.keras.layers import MaxPooling1D\n",
        "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
        "\n",
        "\n",
        "\"\"\"Module to train n-gram model.\n",
        "Vectorizes training and validation texts into n-grams and uses that for\n",
        "training a n-gram model - a simple multi-layer perceptron model. We use n-gram\n",
        "model for text classification when the ratio of number of samples to number of\n",
        "words per sample for the given dataset is very small (<~1500).\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "\n",
        "def train_ngram_model(data,\n",
        "                      learning_rate=1e-3,\n",
        "                      epochs=1000,\n",
        "                      batch_size=128,\n",
        "                      layers=2,\n",
        "                      units=64,\n",
        "                      dropout_rate=0.2):\n",
        "    \"\"\"Trains n-gram model on the given dataset.\n",
        "    # Arguments\n",
        "        data: tuples of training and test texts and labels.\n",
        "        learning_rate: float, learning rate for training model.\n",
        "        epochs: int, number of epochs.\n",
        "        batch_size: int, number of samples per batch.\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of Dense layers in the model.\n",
        "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "    # Raises\n",
        "        ValueError: If validation data has label values which were not seen\n",
        "            in the training data.\n",
        "    \"\"\"\n",
        "    # Get the data.\n",
        "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "    # Verify that validation labels are in the same range as training labels.\n",
        "    num_classes = explore_data.get_num_classes(train_labels)\n",
        "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
        "    if len(unexpected_labels):\n",
        "        raise ValueError('Unexpected label values found in the validation set:'\n",
        "                         ' {unexpected_labels}. Please make sure that the '\n",
        "                         'labels in the validation set are in the same range '\n",
        "                         'as training labels.'.format(\n",
        "                             unexpected_labels=unexpected_labels))\n",
        "\n",
        "    # Vectorize texts.\n",
        "    x_train, x_val = vectorize_data.ngram_vectorize(\n",
        "        train_texts, train_labels, val_texts)\n",
        "\n",
        "    # Create model instance.\n",
        "    model = build_model.mlp_model(layers=layers,\n",
        "                                  units=units,\n",
        "                                  dropout_rate=dropout_rate,\n",
        "                                  input_shape=x_train.shape[1:],\n",
        "                                  num_classes=num_classes)\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    if num_classes == 2:\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=2)]\n",
        "\n",
        "    # Train and validate model.\n",
        "    history = model.fit(\n",
        "            x_train,\n",
        "            train_labels,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            validation_data=(x_val, val_labels),\n",
        "            verbose=2,  # Logs once per epoch.\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('imdb_mlp_model.h5')\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Module to demonstrate hyper-parameter tuning.\n",
        "Trains n-gram model with different combination of hyper-parameters and finds\n",
        "the one that works best.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "\n",
        "data=\"\"\n",
        "FLAGS = None\n",
        "\n",
        "\n",
        "def tune_ngram_model(data):\n",
        "    \"\"\"Tunes n-gram model on the given dataset.\n",
        "    # Arguments\n",
        "        data: tuples of training and test texts and labels.\n",
        "    \"\"\"\n",
        "    # Select parameter values to try.\n",
        "    num_layers = [1, 2, 3]\n",
        "    num_units = [8, 16, 32, 64, 128]\n",
        "\n",
        "    # Save parameter combination and results.\n",
        "    params = {\n",
        "        'layers': [],\n",
        "        'units': [],\n",
        "        'accuracy': [],\n",
        "    }\n",
        "\n",
        "    # Iterate over all parameter combinations.\n",
        "    for layers in num_layers:\n",
        "        for units in num_units:\n",
        "                params['layers'].append(layers)\n",
        "                params['units'].append(units)\n",
        "\n",
        "                accuracy, _ = train_ngram_model(\n",
        "                    data=data,\n",
        "                    layers=layers,\n",
        "                    units=units)\n",
        "                print(('Accuracy: {accuracy}, Parameters: (layers={layers}, '\n",
        "                       'units={units})').format(accuracy=accuracy,\n",
        "                                                layers=layers,\n",
        "                                                units=units))\n",
        "                params['accuracy'].append(accuracy)\n",
        "    _plot_parameters(params)\n",
        "\n",
        "\n",
        "def _plot_parameters(params):\n",
        "    \"\"\"Creates a 3D surface plot of given parameters.\n",
        "    # Arguments\n",
        "        params: dict, contains layers, units and accuracy value combinations.\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca(projection='3d')\n",
        "    ax.plot_trisurf(params['layers'],\n",
        "                    params['units'],\n",
        "                    params['accuracy'],\n",
        "                    cmap=cm.coolwarm,\n",
        "                    antialiased=False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sepcnn_model(blocks,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 embedding_dim,\n",
        "                 dropout_rate,\n",
        "                 pool_size,\n",
        "                 input_shape,\n",
        "                 num_classes,\n",
        "                 num_features,\n",
        "                 use_pretrained_embedding=False,\n",
        "                 is_embedding_trainable=False,\n",
        "                 embedding_matrix=None):\n",
        "    \"\"\"Creates an instance of a separable CNN model.\n",
        "\n",
        "    # Arguments\n",
        "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "        filters: int, output dimension of the layers.\n",
        "        kernel_size: int, length of the convolution window.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "        num_features: int, number of words (embedding input dimension).\n",
        "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
        "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
        "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
        "\n",
        "    # Returns\n",
        "        A sepCNN model instance.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
        "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
        "    if use_pretrained_embedding:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0],\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=is_embedding_trainable))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0]))\n",
        "\n",
        "    for _ in range(blocks-1):\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "    model.add(Dense(op_units, activation=op_activation))\n",
        "    return model\n",
        "\n",
        "\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "\n",
        "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model\n",
        "\n",
        "\"\"\"Module to vectorize data.\n",
        "Converts the given training and validation texts into numerical tensors.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.preprocessing import text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "# Vectorization parameters\n",
        "\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)\n",
        "\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "# One of 'word', 'char'.\n",
        "TOKEN_MODE = 'word'\n",
        "\n",
        "# Minimum document/corpus frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 2\n",
        "\n",
        "# Limit on the length of text sequences. Sequences longer than this\n",
        "# will be truncated.\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
        "    \"\"\"Vectorizes texts as ngram vectors.\n",
        "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        train_labels: np.ndarray, training labels.\n",
        "        val_texts: list, validation text strings.\n",
        "    # Returns\n",
        "        x_train, x_val: vectorized training and validation texts\n",
        "    \"\"\"\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
        "            'dtype': 'int32',\n",
        "            'strip_accents': 'unicode',\n",
        "            'decode_error': 'replace',\n",
        "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
        "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
        "    }\n",
        "    global vectorizer\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    x_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize validation texts.\n",
        "    x_val = vectorizer.transform(val_texts)\n",
        "\n",
        "    # Select top 'k' of the vectorized features.\n",
        "    global selector\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "    selector.fit(x_train, train_labels)\n",
        "    x_train = selector.transform(x_train)\n",
        "    x_val = selector.transform(x_val)\n",
        "\n",
        "    x_train = x_train.astype('float32').toarray()\n",
        "    x_val = x_val.astype('float32').toarray()\n",
        "    return x_train, x_val\n",
        "\n",
        "\n",
        "def sequence_vectorize(train_texts, val_texts):\n",
        "    \"\"\"Vectorizes texts as sequence vectors.\n",
        "    1 text = 1 sequence vector with fixed length.\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        val_texts: list, validation text strings.\n",
        "    # Returns\n",
        "        x_train, x_val, word_index: vectorized training and validation\n",
        "            texts and word index dictionary.\n",
        "    \"\"\"\n",
        "    # Create vocabulary with training texts.\n",
        "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "    # Vectorize training and validation texts.\n",
        "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
        "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "    # Get max sequence length.\n",
        "    max_length = len(max(x_train, key=len))\n",
        "    if max_length > MAX_SEQUENCE_LENGTH:\n",
        "        max_length = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "    # Fix sequence length to max value. Sequences shorter than the length are\n",
        "    # padded in the beginning and sequences longer are truncated\n",
        "    # at the beginning.\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
        "    return x_train, x_val, tokenizer.word_index\n",
        "\n",
        "\n",
        "\"\"\"Module to create model.\n",
        "Helper functions to create a multi-layer perceptron model and a separable CNN\n",
        "model. These functions take the model hyper-parameters as input. This will\n",
        "allow us to create model instances with slightly varying architectures.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.layers import Embedding\n",
        "from tensorflow.python.keras.layers import SeparableConv1D\n",
        "from tensorflow.python.keras.layers import MaxPooling1D\n",
        "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
        "\n",
        "\n",
        "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model\n",
        "\n",
        "\n",
        "def sepcnn_model(blocks,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 embedding_dim,\n",
        "                 dropout_rate,\n",
        "                 pool_size,\n",
        "                 input_shape,\n",
        "                 num_classes,\n",
        "                 num_features,\n",
        "                 use_pretrained_embedding=False,\n",
        "                 is_embedding_trainable=False,\n",
        "                 embedding_matrix=None):\n",
        "    \"\"\"Creates an instance of a separable CNN model.\n",
        "    # Arguments\n",
        "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "        filters: int, output dimension of the layers.\n",
        "        kernel_size: int, length of the convolution window.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "        num_features: int, number of words (embedding input dimension).\n",
        "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
        "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
        "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
        "    # Returns\n",
        "        A sepCNN model instance.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
        "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
        "    if use_pretrained_embedding:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0],\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=is_embedding_trainable))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0]))\n",
        "\n",
        "    for _ in range(blocks-1):\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "    model.add(Dense(op_units, activation=op_activation))\n",
        "    return model\n",
        "\n",
        "\n",
        "def _get_last_layer_units_and_activation(num_classes):\n",
        "    \"\"\"Gets the # units and activation function for the last network layer.\n",
        "    # Arguments\n",
        "        num_classes: int, number of classes.\n",
        "    # Returns\n",
        "        units, activation values.\n",
        "    \"\"\"\n",
        "    if num_classes == 2:\n",
        "        activation = 'sigmoid'\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = 'softmax'\n",
        "        units = num_classes\n",
        "    return units, activation\n",
        "\n",
        "\"\"\"Module to load data.\n",
        "Consists of functions to load data from four different datasets (IMDb, Rotten\n",
        "Tomatoes, Tweet Weather, Amazon Reviews). Each of these functions do the\n",
        "following:\n",
        "    - Read the required fields (texts and labels).\n",
        "    - Do any pre-processing if required. For example, make sure all label\n",
        "        values are in range [0, num_classes-1].\n",
        "    - Split the data into training and validation sets.\n",
        "    - Shuffle the training data.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
        "    \"\"\"Loads the Imdb movie reviews sentiment analysis dataset.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        seed: int, seed for randomizer.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 25000\n",
        "        Number of test samples: 25000\n",
        "        Number of categories: 2 (0 - negative, 1 - positive)\n",
        "    # References\n",
        "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
        "        Download and uncompress archive from:\n",
        "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    \"\"\"\n",
        "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
        "\n",
        "    # Load the training data\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
        "        for fname in sorted(os.listdir(train_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(train_path, fname)) as f:\n",
        "                    train_texts.append(f.read())\n",
        "                train_labels.append(0 if category == 'neg' else 1)\n",
        "\n",
        "    # Load the validation data.\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
        "        for fname in sorted(os.listdir(test_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(test_path, fname)) as f:\n",
        "                    test_texts.append(f.read())\n",
        "                test_labels.append(0 if category == 'neg' else 1)\n",
        "\n",
        "    # Shuffle the training data and labels.\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_texts)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_labels)\n",
        "\n",
        "    return ((train_texts, np.array(train_labels)),\n",
        "            (test_texts, np.array(test_labels)))\n",
        "\n",
        "\n",
        "def load_tweet_weather_topic_classification_dataset(data_path,\n",
        "                                                    validation_split=0.2,\n",
        "                                                    seed=123):\n",
        "    \"\"\"Loads the tweet weather topic classification dataset.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        validation_split: float, percentage of data to use for validation.\n",
        "        seed: int, seed for randomizer.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 62356\n",
        "        Number of test samples: 15590\n",
        "        Number of topics: 15\n",
        "    # References\n",
        "        https://www.kaggle.com/c/crowdflower-weather-twitter/data\n",
        "        Download from:\n",
        "        https://www.kaggle.com/c/3586/download/train.csv\n",
        "    \"\"\"\n",
        "    columns = [1] + [i for i in range(13, 28)]  # 1 - text, 13-28 - topics.\n",
        "    data = _load_and_shuffle_data(data_path, 'train.csv', columns, seed)\n",
        "\n",
        "    # Get tweet text and the max confidence score for the weather types.\n",
        "    texts = list(data['tweet'])\n",
        "    weather_data = data.iloc[:, 1:]\n",
        "\n",
        "    labels = []\n",
        "    for i in range(len(texts)):\n",
        "        # Pick topic with the max confidence score.\n",
        "        labels.append(np.argmax(list(weather_data.iloc[i, :].values)))\n",
        "\n",
        "    return _split_training_and_validation_sets(\n",
        "        texts, np.array(labels), validation_split)\n",
        "\n",
        "\n",
        "def load_rotten_tomatoes_sentiment_analysis_dataset(data_path,\n",
        "                                                    validation_split=0.2,\n",
        "                                                    seed=123):\n",
        "    \"\"\"Loads the rotten tomatoes sentiment analysis dataset.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        validation_split: float, percentage of data to use for validation.\n",
        "        seed: int, seed for randomizer.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 124848\n",
        "        Number of test samples: 31212\n",
        "        Number of categories: 5 (0 - negative, 1 - somewhat negative,\n",
        "                2 - neutral, 3 - somewhat positive, 4 - positive)\n",
        "    # References\n",
        "        https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data\n",
        "        Download and uncompress archive from:\n",
        "        https://www.kaggle.com/c/3810/download/train.tsv.zip\n",
        "    \"\"\"\n",
        "    columns = (2, 3)  # 2 - Phrases, 3 - Sentiment.\n",
        "    data = _load_and_shuffle_data(data_path, 'train.tsv', columns, seed, '\\t')\n",
        "\n",
        "    # Get the review phrase and sentiment values.\n",
        "    texts = list(data['Phrase'])\n",
        "    labels = np.array(data['Sentiment'])\n",
        "    return _split_training_and_validation_sets(texts, labels, validation_split)\n",
        "\n",
        "\n",
        "def load_amazon_reviews_sentiment_analysis_dataset(data_path, seed=123):\n",
        "    \"\"\"Loads the amazon reviews sentiment analysis dataset.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        seed: int, seed for randomizer.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 3000000\n",
        "        Number of test samples: 650000\n",
        "        Number of categories: 5\n",
        "    # References\n",
        "        Zhang et al., https://arxiv.org/abs/1509.01626\n",
        "        Download and uncompress archive from:\n",
        "                https://drive.google.com/open?id=0Bz8a_Dbh9QhbZVhsUnRWRDhETzA\n",
        "    \"\"\"\n",
        "    columns = (0, 1, 2)  # 0 - label, 1 - title, 2 - body.\n",
        "    train_data = _load_and_shuffle_data(\n",
        "            data_path, 'train.csv', columns, seed, header=None)\n",
        "\n",
        "    test_data_path = os.path.join(data_path, 'test.csv')\n",
        "    test_data = pd.read_csv(test_data_path, usecols=columns, header=None)\n",
        "\n",
        "    # Get train and test labels. Replace label value 5 with value 0.\n",
        "    train_labels = np.array(train_data.iloc[:, 0])\n",
        "    train_labels[train_labels == 5] = 0\n",
        "    test_labels = np.array(test_data.iloc[:, 0])\n",
        "    test_labels[test_labels == 5] = 0\n",
        "\n",
        "    # Get train and test texts.\n",
        "    train_texts = []\n",
        "    for index, row in train_data.iterrows():\n",
        "        train_texts.append(_get_amazon_review_text(row))\n",
        "    test_texts = []\n",
        "    for index, row in test_data.iterrows():\n",
        "        test_texts.append(_get_amazon_review_text(row))\n",
        "\n",
        "    return ((train_texts, train_labels), (test_texts, test_labels))\n",
        "\n",
        "\n",
        "def _get_amazon_review_text(row):\n",
        "    \"\"\"Gets the Amazon review text given row data.\n",
        "    # Arguments\n",
        "        row: pandas row data from Amazon review dataset.\n",
        "    # Returns:\n",
        "        string, text corresponding to the row.\n",
        "    \"\"\"\n",
        "    title = ''\n",
        "    if type(row[1]) == str:\n",
        "        title = row[1].replace('\\\\n', '\\n').replace('\\\\\"', '\"')\n",
        "    body = ''\n",
        "    if type(row[2]) == str:\n",
        "        body = row[2].replace('\\\\n', '\\n').replace('\\\\\"', '\"')\n",
        "    return title + ', ' + body\n",
        "\n",
        "\n",
        "def _load_and_shuffle_data(data_path,\n",
        "                           file_name,\n",
        "                           cols,\n",
        "                           seed,\n",
        "                           separator=',',\n",
        "                           header=0):\n",
        "    \"\"\"Loads and shuffles the dataset using pandas.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        file_name: string, name of the data file.\n",
        "        cols: list, columns to load from the data file.\n",
        "        seed: int, seed for randomizer.\n",
        "        separator: string, separator to use for splitting data.\n",
        "        header: int, row to use as data header.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    data_path = os.path.join(data_path, file_name)\n",
        "    data = pd.read_csv(data_path, usecols=cols, sep=separator, header=header)\n",
        "    return data.reindex(np.random.permutation(data.index))\n",
        "\n",
        "\n",
        "def _split_training_and_validation_sets(texts, labels, validation_split):\n",
        "    \"\"\"Splits the texts and labels into training and validation sets.\n",
        "    # Arguments\n",
        "        texts: list, text data.\n",
        "        labels: list, label data.\n",
        "        validation_split: float, percentage of data to use for validation.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "    \"\"\"\n",
        "    num_training_samples = int((1 - validation_split) * len(texts))\n",
        "    return ((texts[:num_training_samples], labels[:num_training_samples]),\n",
        "            (texts[num_training_samples:], labels[num_training_samples:]))\n",
        "    \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_num_words_per_sample(sample_texts):\n",
        "    \"\"\"Returns the median number of words per sample given corpus.\n",
        "\n",
        "    # Arguments\n",
        "        sample_texts: list, sample texts.\n",
        "\n",
        "    # Returns\n",
        "        int, median number of words per sample.\n",
        "    \"\"\"\n",
        "    num_words = [len(s.split()) for s in sample_texts]\n",
        "    return np.median(num_words)\n",
        "\n",
        "def plot_sample_length_distribution(sample_texts):\n",
        "    \"\"\"Plots the sample length distribution.\n",
        "\n",
        "    # Arguments\n",
        "        samples_texts: list, sample texts.\n",
        "    \"\"\"\n",
        "    plt.hist([len(s) for s in sample_texts], 50)\n",
        "    plt.xlabel('Length of a sample')\n",
        "    plt.ylabel('Number of samples')\n",
        "    plt.title('Sample length distribution')\n",
        "    plt.show()\n",
        "\n",
        "\"\"\"Module to explore data.\n",
        "Contains functions to help study, visualize and understand datasets.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "def get_num_classes(labels):\n",
        "    \"\"\"Gets the total number of classes.\n",
        "    # Arguments\n",
        "        labels: list, label values.\n",
        "            There should be at lease one sample for values in the\n",
        "            range (0, num_classes -1)\n",
        "    # Returns\n",
        "        int, total number of classes.\n",
        "    # Raises\n",
        "        ValueError: if any label value in the range(0, num_classes - 1)\n",
        "            is missing or if number of classes is <= 1.\n",
        "    \"\"\"\n",
        "    num_classes = max(labels) + 1\n",
        "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
        "    if len(missing_classes):\n",
        "        raise ValueError('Missing samples with label value(s) '\n",
        "                         '{missing_classes}. Please make sure you have '\n",
        "                         'at least one sample for every label value '\n",
        "                         'in the range(0, {max_class})'.format(\n",
        "                            missing_classes=missing_classes,\n",
        "                            max_class=num_classes - 1))\n",
        "\n",
        "    if num_classes <= 1:\n",
        "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
        "                         'Please make sure there are at least two classes '\n",
        "                         'of samples'.format(num_classes=num_classes))\n",
        "    return num_classes\n",
        "\n",
        "\n",
        "def get_num_words_per_sample(sample_texts):\n",
        "    \"\"\"Gets the median number of words per sample given corpus.\n",
        "    # Arguments\n",
        "        sample_texts: list, sample texts.\n",
        "    # Returns\n",
        "        int, median number of words per sample.\n",
        "    \"\"\"\n",
        "    num_words = [len(s.split()) for s in sample_texts]\n",
        "    return np.median(num_words)\n",
        "\n",
        "\n",
        "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
        "                                          ngram_range=(1, 2),\n",
        "                                          num_ngrams=50):\n",
        "    \"\"\"Plots the frequency distribution of n-grams.\n",
        "    # Arguments\n",
        "        samples_texts: list, sample texts.\n",
        "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
        "            Min and mplt are the lower and upper bound values for the range.\n",
        "        num_ngrams: int, number of n-grams to plot.\n",
        "            Top `num_ngrams` frequent n-grams will be plotted.\n",
        "    \"\"\"\n",
        "    # Create args required for vectorizing.\n",
        "    kwargs = {\n",
        "            'ngram_range': (1, 1),\n",
        "            'dtype': 'int32',\n",
        "            'strip_accents': 'unicode',\n",
        "            'decode_error': 'replace',\n",
        "            'analyzer': 'word',  # Split text into word tokens.\n",
        "    }\n",
        "    vectorizer = CountVectorizer(**kwargs)\n",
        "\n",
        "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
        "    # idxices). This also converts every text to an array the length of\n",
        "    # vocabulary, where every element idxicates the count of the n-gram\n",
        "    # corresponding at that idxex in vocabulary.\n",
        "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
        "\n",
        "    # This is the list of all n-grams in the index order from the vocabulary.\n",
        "    all_ngrams = list(vectorizer.get_feature_names())\n",
        "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
        "    # ngrams = all_ngrams[:num_ngrams]\n",
        "\n",
        "    # Add up the counts per n-gram ie. column-wise\n",
        "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
        "\n",
        "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
        "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
        "        zip(all_counts, all_ngrams), reverse=True)])\n",
        "    ngrams = list(all_ngrams)[:num_ngrams]\n",
        "    counts = list(all_counts)[:num_ngrams]\n",
        "\n",
        "    idx = np.arange(num_ngrams)\n",
        "    plt.bar(idx, counts, width=0.8, color='b')\n",
        "    plt.xlabel('N-grams')\n",
        "    plt.ylabel('Frequencies')\n",
        "    plt.title('Frequency distribution of n-grams')\n",
        "    plt.xticks(idx, ngrams, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_sample_length_distribution(sample_texts):\n",
        "    \"\"\"Plots the sample length distribution.\n",
        "    # Arguments\n",
        "        samples_texts: list, sample texts.\n",
        "    \"\"\"\n",
        "    plt.hist([len(s) for s in sample_texts], 50)\n",
        "    plt.xlabel('Length of a sample')\n",
        "    plt.ylabel('Number of samples')\n",
        "    plt.title('Sample length distribution')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_class_distribution(labels):\n",
        "    \"\"\"Plots the class distribution.\n",
        "    # Arguments\n",
        "        labels: list, label values.\n",
        "            There should be at lease one sample for values in the\n",
        "            range (0, num_classes -1)\n",
        "    \"\"\"\n",
        "    num_classes = get_num_classes(labels)\n",
        "    count_map = Counter(labels)\n",
        "    counts = [count_map[i] for i in range(num_classes)]\n",
        "    idx = np.arange(num_classes)\n",
        "    plt.bar(idx, counts, width=0.8, color='b')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of samples')\n",
        "    plt.title('Class distribution')\n",
        "    plt.xticks(idx, idx)    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiQw11UadxFs"
      },
      "source": [
        "#Sentiment analysis in Language using Deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPRYwJcfuOCU"
      },
      "source": [
        "#Algorithm for Data Preparation and Model Building\n",
        "\n",
        "```\n",
        "1. Calculate the number of samples/number of words per sample ratio.\n",
        "2. If this ratio is less than 1500, tokenize the text as n-grams and use a\n",
        "simple multi-layer perceptron (MLP) model to classify them (left branch in the\n",
        "flowchart below):\n",
        "  a. Split the samples into word n-grams; convert the n-grams into vectors.\n",
        "  b. Score the importance of the vectors and then select the top 20K using the scores.\n",
        "  c. Build an MLP model.\n",
        "3. If the ratio is greater than 1500, tokenize the text as sequences and use a\n",
        "   sepCNN model to classify them (right branch in the flowchart below):\n",
        "  a. Split the samples into words; select the top 20K words based on their frequency.\n",
        "  b. Convert the samples into word sequence vectors.\n",
        "  c. If the original number of samples/number of words per sample ratio is less\n",
        "     than 15K, using a fine-tuned pre-trained embedding with the sepCNN\n",
        "     model will likely provide the best results.\n",
        "4. Measure the model performance with different hyperparameter values to find\n",
        "   the best model configuration for the dataset.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exR2sDRnvxp8"
      },
      "source": [
        "<img src=\"https://upscfever.com/datasets/TextClassificationFlowchart.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwHfN66RYbic"
      },
      "source": [
        "IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvbneDj3d1wp"
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -O imdb.tar.gz\n",
        "import tarfile\n",
        "with tarfile.open('imdb.tar.gz', 'r:gz') as tar:\n",
        "    tar.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZtJeGQHfJfq"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = load_imdb_sentiment_analysis_dataset(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_DLSogXfoVV"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6pVaffsjrPP"
      },
      "source": [
        "get_num_classes(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_ksjdPSjyMU"
      },
      "source": [
        "#W = 174 (Gets the median number of words per sample given corpus.)\n",
        "#S = 25000 (Total rows)\n",
        "#S/W = 25000/174 <<< 1500\n",
        "get_num_words_per_sample(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZoTDn3w3-9j"
      },
      "source": [
        "#Tokenization\n",
        "We have found that tokenizing into word unigrams + bigrams provides good accuracy while taking less compute time.\n",
        "\n",
        "#Vectorization\n",
        "Once we have split our text samples into n-grams, we need to turn these n-grams into numerical vectors that our machine learning models can process. The example below shows the indexes assigned to the unigrams and bigrams generated for two texts.\n",
        "\n",
        "```\n",
        "Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n",
        "Index assigned for every token: {'the': 7, 'mouse': 2, 'ran': 4, 'up': 10,\n",
        "  'clock': 0, 'the mouse': 9, 'mouse ran': 3, 'ran up': 6, 'up the': 11, 'the\n",
        "clock': 8, 'down': 1, 'ran down': 5}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfTSgkgW4bAL"
      },
      "source": [
        "#Once indexes are assigned to the n-grams, we typically vectorize using one of the following options.\n",
        "\n",
        "```\n",
        "Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n",
        "Index assigned for every token: {'the': 7, 'mouse': 2, 'ran': 4, 'up': 10,\n",
        "  'clock': 0, 'the mouse': 9, 'mouse ran': 3, 'ran up': 6, 'up the': 11, 'the\n",
        "clock': 8, 'down': 1, 'ran down': 5}\n",
        "```\n",
        "\n",
        "1. One hot encoding:'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "2. Count encoding: 'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1]\n",
        "\n",
        "3. TFIDF encoding: 'The mouse ran up the clock' = [0.33, 0, 0.23, 0.23, 0.23, 0, 0.33, 0.47, 0.33,0.23, 0.33, 0.33]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXvFgOpvnsvT"
      },
      "source": [
        "#The mouse ran up and ran down - [0,1,1,1,1,1,1,1,0,1,1,0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWs9JHluoz3Q"
      },
      "source": [
        "#TFIDF = Term frequency / Inverse document frequency "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRIVmddSpSGw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw0z4QODp4zM"
      },
      "source": [
        "X_train_vec, X_val_vec = ngram_vectorize(X_train, y_train, X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eHA7FB7qgyO"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIqb9An-qbNf"
      },
      "source": [
        "X_train_vec[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8HbZgtpqlS2"
      },
      "source": [
        "X_train_vec.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VqyCW0urQUs"
      },
      "source": [
        "from keras.models import Sequential\n",
        "imdbnn = Sequential()\n",
        "imdbnn.add(Dense(units=512, activation='relu', input_dim=20000))\n",
        "imdbnn.add(Dense(units=1, activation='sigmoid'))\n",
        "imdbnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0aRHBp-rxwH"
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "rd = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "filepath='weight_{epoch:02d}_{val_loss:.2f}.h5'\n",
        "mc = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phY61GBQsYWT"
      },
      "source": [
        "history = imdbnn.fit(x=X_train_vec, y=y_train, epochs=100, validation_data=(X_val_vec, y_val), callbacks=[rd,es,mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49-feSt3u-qm"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy on train and validation set')\n",
        "plt.legend(['Train','Validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDWz5dIKvivM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss on train and validation set')\n",
        "plt.legend(['Train','Validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dT6V_z-vvp9"
      },
      "source": [
        "X_train_vec, X_test_vec = ngram_vectorize(X_train, y_train, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-qw7LNTwNJj"
      },
      "source": [
        "imdbnn.evaluate(X_test_vec, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxHSG8Ua2MT"
      },
      "source": [
        "Deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6zi6IKha5JL"
      },
      "source": [
        "import numpy as np\n",
        "from flask import Flask, request, render_template, jsonify\n",
        "!pip install flask_ngrok\n",
        "from flask_ngrok import run_with_ngrok\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsWhOYKAbad9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bDpK1TjbcOT"
      },
      "source": [
        "Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpEdSL-NbglU"
      },
      "source": [
        "!mkdir '/content/templates' # to create HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHhcoLkrfAOg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM89b_5cfmgg"
      },
      "source": [
        "integrate index.html with imbdnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2VeVxEDfsFT"
      },
      "source": [
        "# Create object named app of flask, ou can also use different web server like Amazon web services\n",
        "app = Flask(__name__) # with flask it is possible to launch your model as website\n",
        "#running it\n",
        "run_with_ngrok(app) # app is object and flask is web server\n",
        "\n",
        "#Create and display homepage\n",
        "@app.route('/')\n",
        "def homepage():\n",
        "  return render_template('index.html')\n",
        "\n",
        "#Create prediction\n",
        "@app.route('/', methods=['POST'])\n",
        "def predictions():\n",
        "  received = request.form['reviews']  # we  storing data in received variable\n",
        "  #Creating numpy array\n",
        "  received=[received]\n",
        "\n",
        "  #tokennize and vectorize - ngram_vectorize()\n",
        "\n",
        "  received_vec = vectorizer.transform(received)\n",
        "  received_vec = selector.transform(received_vec)\n",
        "  received_vec = received_vec.astype('float32').toarray()\n",
        "\n",
        " # _, received_vec = ngram_vectorize(X_train, y_train, received)   no need of this line\n",
        "\n",
        "  # NN Prediction\n",
        "  results = imdbnn.predict_classes(received_vec)\n",
        "\n",
        "  #Check prediction\n",
        "  if results.astype('int32') == 0:\n",
        "    results = 'negative sentiment'\n",
        "  else:\n",
        "    results = 'positive sentiment'\n",
        "  # print(received)\n",
        "  return render_template('index.html', answer=results) \n",
        "\n",
        "#Execute functions\n",
        "\n",
        "if __name__=='__main__':\n",
        "  app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF90Gd5voEHx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}